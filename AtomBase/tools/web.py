"""
Web Tools for Atomik
Based on AtomAgent's robust web capabilities.
Allows Atomik to research, read pages, and get news.
"""
import requests
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from duckduckgo_search import DDGS
from utils.logger import get_logger

logger = get_logger()

# GÃ¼venilir siteler listesi (Opsiyonel filtreleme iÃ§in)
TRUSTED_DOMAINS = [
    "github.com", "stackoverflow.com", "docs.python.org", "pypi.org",
    "developer.mozilla.org", "w3schools.com", "geeksforgeeks.org",
    "medium.com", "dev.to", "wikipedia.org"
]

def _clean_text(text: str, max_length: int = 1500) -> str:
    """Metni temizler ve kÄ±saltÄ±r."""
    text = ' '.join(text.split())
    if len(text) > max_length:
        text = text[:max_length] + "... (devamÄ± kesildi)"
    return text

@tool
def web_search(query: str, max_results: int = 5) -> str:
    """
    Internet Ã¼zerinde arama yapar.
    Bilgi eksikliÄŸi olduÄŸunda veya gÃ¼ncel bilgi gerektiÄŸinde kullanÄ±n.
    
    Args:
        query: Arama sorgusu (Ã¶rn: "python request library usage")
        max_results: SonuÃ§ sayÄ±sÄ± (varsayÄ±lan 5)
    """
    logger.info(f"Web search: {query}")
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=max_results))
        
        if not results:
            return "SonuÃ§ bulunamadÄ±."
            
        output = [f"ğŸ” '{query}' iÃ§in sonuÃ§lar:\n"]
        for i, r in enumerate(results, 1):
            output.append(f"{i}. {r.get('title', 'BaÅŸlÄ±k Yok')}")
            output.append(f"   ğŸ”— {r.get('href', '')}")
            output.append(f"   ğŸ“ {r.get('body', '')}\n")
        
        output.append("\nğŸ’¡ Detay iÃ§in `visit_webpage` kullanabilirsin.")
        return "\n".join(output)
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return f"Arama hatasÄ±: {e}"

@tool
def visit_webpage(url: str) -> str:
    """
    Bir web sayfasÄ±nÄ± ziyaret eder ve iÃ§eriÄŸini okur.
    `web_search` sonucundaki linkleri okumak iÃ§in kullanÄ±n.
    
    Args:
        url: Ziyaret edilecek URL
    """
    logger.info(f"Visiting: {url}")
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Gereksiz tagleri temizle
        for tag in soup(["script", "style", "nav", "footer", "iframe", "noscript"]):
            tag.decompose()
            
        # Ana iÃ§eriÄŸi bulmaya Ã§alÄ±ÅŸ
        main_content = (
            soup.find("main") or 
            soup.find("article") or 
            soup.find("body")
        )
        
        if not main_content:
            main_content = soup
            
        text = main_content.get_text(separator='\n', strip=True)
        cleaned_text = _clean_text(text)
        
        return f"ğŸ“„ {url}\n\n{cleaned_text}"
        
    except Exception as e:
        logger.error(f"Visit failed: {url} - {e}")
        return f"Sayfa okuma hatasÄ±: {e}"

@tool
def get_news(topic: str = "") -> str:
    """
    GÃ¼ncel haberleri getirir.
    
    Args:
        topic: Haber konusu (boÅŸ bÄ±rakÄ±lÄ±rsa genel teknoloji)
    """
    logger.info(f"News: {topic}")
    query = topic if topic else "technology software ai news"
    try:
        with DDGS() as ddgs:
            news = list(ddgs.news(query, max_results=5))
            
        if not news:
            return "Haber bulunamadÄ±."
            
        output = [f"ğŸ“° GÃ¼ncel Haberler ({query}):\n"]
        for n in news:
            output.append(f"â€¢ {n.get('title')}")
            output.append(f"  {n.get('date', '')} - {n.get('source', '')}")
            output.append(f"  ğŸ”— {n.get('url')}\n")
            
        return "\n".join(output)
    except Exception as e:
        return f"Haber alma hatasÄ±: {e}"
